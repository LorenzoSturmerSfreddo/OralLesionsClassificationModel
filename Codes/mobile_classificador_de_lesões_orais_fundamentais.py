# -*- coding: utf-8 -*-
"""Mobile - Classificador de Lesões Orais Fundamentais.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14U17uMQ2fl6wI2qhTUgvBf4bxzLfOVVE

#%% BIBLIOTECAS
"""

import matplotlib.pyplot as plt
import numpy as np
import os
import PIL
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, GlobalAveragePooling2D, Dropout

import sklearn
import sklearn.metrics

"""#%% LEITURA DAS IMAGENS

"""

batch_size = 32
SIZE=(299,299)
img_height = SIZE[0]
img_width = SIZE[1]

data_dir = "/content/drive/MyDrive/TCC/BANCO_V4"

train_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.3,
  subset="training",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)


val_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset="validation",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)

class_names = train_ds.class_names
print(class_names)

"""#%% DESEMPENHO

"""

AUTOTUNE = tf.data.AUTOTUNE

#train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

"""#%% PADRONIZACAO

"""

#normalization_layer = layers.Rescaling(1./299)

#normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))
#image_batch, labels_batch = next(iter(normalized_ds))
#first_image = image_batch[0]
# Notice the pixel values are now in `[0,1]`.
#print(np.min(first_image), np.max(first_image))

"""# %% ***** MODELO DA REDE *****

"""

CLASSES=6 #Numero de 'estrelas' atribuidas a cada botijao

CNN_ARCHITECTURE = "Mobile"

if CNN_ARCHITECTURE=="Mobile":
    mobile = tf.keras.applications.mobilenet.MobileNet(weights='imagenet',
                                                       include_top= False,
                                                       input_shape=(SIZE[0],SIZE[1],3))

    train_layer = mobile.layers[-1]
    train_index = mobile.layers.index(train_layer)

    # Percorre as camadas a seguir e aplica a operação no novo input
    for layer in mobile.layers[1:train_index+1]:
        # não desliga treinamento para BatchNorm
        if isinstance(layer, BatchNormalization):
            layer.trainable=True
        else: # congela as outras camadas
            layer.trainable=False

    # Usa average pooling para diminuir a resolução para 1
    x = GlobalAveragePooling2D()(layer.output)
    x = Dropout(0.1)(x)
    x=Dense(16,activation='relu')(x)
    x=Dense(8,activation='relu')(x)
    output = Dense(units=CLASSES, activation='softmax')(x)

    model = Model(inputs=mobile.input, outputs=output)

    model.summary()

if CNN_ARCHITECTURE=="Resnet":
    resnet = tf.keras.applications.resnet50.ResNet50(weights='imagenet',
                                                     include_top= False,
                                                     input_shape=(SIZE[0],SIZE[1],3))

    train_layer = resnet.layers[-1]
    train_index = resnet.layers.index(train_layer)

    for layer in resnet.layers[1:train_index+1]:
        # não desliga treinamento para BatchNorm
        #if isinstance(layer, BatchNormalization):
            layer.trainable=True
        #else:
        #    layer.trainable=False

    # Usa average pooling para diminuir a resolução para 1
    x=GlobalAveragePooling2D()(layer.output)
    x = Dropout(0.1)(x)
    x=Dense(16,activation='relu')(x)
    x=Dense(8,activation='relu')(x)
    output = Dense(units=CLASSES, activation='softmax')(x)

    model = Model(inputs=resnet.input, outputs=output)

    model.summary()

if CNN_ARCHITECTURE=="Inception":
    inception = tf.keras.applications.InceptionV3(weights='imagenet',
                                                  include_top= False,
                                                  input_shape=(SIZE[0],SIZE[1],3))

    train_layer = inception.layers[-1]
    train_index = inception.layers.index(train_layer)

    for layer in inception.layers[1:train_index+1]:
        # não desliga treinamento para BatchNorm
        if isinstance(layer, BatchNormalization):
            layer.trainable=True
        else:
            layer.trainable=False

    # Usa average pooling para diminuir a resolução para 1
    x=GlobalAveragePooling2D()(layer.output)
    x = Dropout(0.1)(x)
    x=Dense(16,activation='relu')(x)
    x=Dense(8,activation='relu')(x)
    output = Dense(units=CLASSES, activation='softmax')(x)

    model = Model(inputs=inception.input, outputs=output)

    model.summary()

"""#%% COMPILACAO"""

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics=['accuracy'])

"""#%% TREINAMENTO"""

epochs=50
history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs
)

"""#%% RESULTADOS

"""

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

# Matriz de confusão para os dados de validação
y_true = tf.concat([y for x, y in val_ds], axis=0)

ypred = model.predict(val_ds)

y_pred = [np.argmax(probas) for probas in ypred]

cmat = sklearn.metrics.confusion_matrix(y_true=y_true, y_pred=y_pred, normalize= "true")

FP = cmat.sum(axis=0) - np.diag(cmat)
FN = cmat.sum(axis=1) - np.diag(cmat)
TP = np.diag(cmat)
TN = cmat.sum() - (FP + FN + TP)

# Sensitivity, hit rate, recall, or true positive rate
SENSITIVITY = TP/(TP+FN)
# Specificity or true negative rate
SPECIFICITY = TN/(TN+FP)
# Precision or positive predictive value
PRECISION = TP/(TP+FP)
# Overall accuracy
ACCURACY = (TP+TN)/(TP+FP+FN+TN)
# F1-Score
F1_SCORE = 2*(PRECISION*SENSITIVITY/(PRECISION+SENSITIVITY))


disp = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=cmat,
                                display_labels=class_names)

disp.plot(cmap = "Blues")
fig = disp.ax_.get_figure()
fig.set_figwidth(8)
fig.set_figheight(8)
plt.ylabel("Classe")
plt.xlabel("Valor predito")
plt.title("Matriz de Confusão Validação")

print("ACCURACY: ",ACCURACY)
print("SENSITIVITY: ",SENSITIVITY)
print("SPECIFICITY: ",SPECIFICITY)
print("PRECISION: ",PRECISION)
print("F1-SCORE: ",F1_SCORE)

"""# %% ***** GRAD CAM *****

"""

GRAD_CAM = 1

if GRAD_CAM:

    # Display
    from IPython.display import Image, display
    import matplotlib.pyplot as plt
    import matplotlib.cm as cm

    img_size = SIZE

    if CNN_ARCHITECTURE=="Mobile":
        model_builder = keras.applications.mobilenet.MobileNet
        preprocess_input = keras.applications.mobilenet.preprocess_input
        decode_predictions = keras.applications.mobilenet.decode_predictions
        last_conv_layer_name = "conv_pw_13_relu" #Ultima camada da Mobile

    if CNN_ARCHITECTURE=="Resnet":
        model_builder = keras.applications.resnet50.ResNet50
        preprocess_input = keras.applications.resnet50.preprocess_input
        decode_predictions = keras.applications.resnet50.decode_predictions
        last_conv_layer_name = "conv5_block3_out" #Ultima camada da Resnet

    if CNN_ARCHITECTURE=="Inception":
        model_builder = keras.applications.inception_v3.InceptionV3
        preprocess_input = keras.applications.inception_v3.preprocess_input
        decode_predictions = keras.applications.inception_v3.decode_predictions
        last_conv_layer_name = "mixed10" #Ultima camada da Inception


    #nomeArquivo = "\\stenosis\\stenosis_grade0.4_length_43_502.png"
    #nomeArquivo = "\\normal\\normal_10.png"
    #nomeArquivo = "/content/drive/MyDrive/TCC/BANCO_V4/CROP NODULO-BOLHA/2603NODULO._2_crop.jpg"
    #nomeArquivo = "/content/drive/MyDrive/TCC/BANCO_V4/CROP ULCERA-EROSÃO/1651ULCERA._2_crop.jpg"
    nomeArquivo = "/content/drive/MyDrive/TCC/BANCO_V4/CROP PLACA-MANCHA/1358 MANCHA._2_crop.jpg"

    img_path = nomeArquivo

    #image = np.array(tf.keras.preprocessing.image.load_img(img_path, target_size=(SIZE[0], SIZE[1], 3)))

    def get_img_array(img_path, size):
        # `img` is a PIL image of size 299x299
        img = tf.keras.preprocessing.image.load_img(img_path, target_size=size)
        # `array` is a float32 Numpy array of shape (299, 299, 3)
        array = tf.keras.preprocessing.image.img_to_array(img)
        # We add a dimension to transform our array into a "batch"
        # of size (1, 299, 299, 3)
        array = np.expand_dims(array, axis=0)
        return array

    def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
        # First, we create a model that maps the input image to the activations
        # of the last conv layer as well as the output predictions
        grad_model = keras.models.Model(
            model.inputs, [model.get_layer(last_conv_layer_name).output, model.output]
        )

        # Then, we compute the gradient of the top predicted class for our input image
        # with respect to the activations of the last conv layer
        with tf.GradientTape() as tape:
            last_conv_layer_output, preds = grad_model(img_array)
            if pred_index is None:
                pred_index = tf.argmax(preds[0])
            class_channel = preds[:, pred_index]

        # This is the gradient of the output neuron (top predicted or chosen)
        # with regard to the output feature map of the last conv layer
        grads = tape.gradient(class_channel, last_conv_layer_output)

        # This is a vector where each entry is the mean intensity of the gradient
        # over a specific feature map channel
        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

        # We multiply each channel in the feature map array
        # by "how important this channel is" with regard to the top predicted class
        # then sum all the channels to obtain the heatmap class activation
        last_conv_layer_output = last_conv_layer_output[0]
        heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]
        heatmap = tf.squeeze(heatmap)

        # For visualization purpose, we will also normalize the heatmap between 0 & 1
        heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
        return heatmap.numpy()

    # Prepare image
    img_array = preprocess_input(get_img_array(img_path, size=img_size))

    # Make model
    model = model_builder(weights="imagenet")

    # Remove last layer's softmax
    #model.layers[-1].activation = None

    # Print what the top predicted class is
    img_array = tf.image.resize(img_array, (224, 224))
    preds = model.predict(img_array)
    print("Predicted:", decode_predictions(preds, top=1)[0])

    # Generate class activation heatmap
    heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)

    # Display heatmap
    plt.matshow(heatmap)
    plt.show()

    def save_and_display_gradcam(img_path, heatmap, cam_path="cam.jpg", alpha=0.4):
        # Load the original image
        img = tf.keras.preprocessing.image.load_img(img_path)
        img = tf.keras.preprocessing.image.img_to_array(img)

        # Rescale heatmap to a range 0-255
        heatmap = np.uint8(255 * heatmap)

        # Use jet colormap to colorize heatmap
        jet = cm.get_cmap("jet")

        # Use RGB values of the colormap
        jet_colors = jet(np.arange(256))[:, :3]
        jet_heatmap = jet_colors[heatmap]

        # Create an image with RGB colorized heatmap
        jet_heatmap = tf.keras.preprocessing.image.array_to_img(jet_heatmap)
        jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))
        jet_heatmap = tf.keras.preprocessing.image.img_to_array(jet_heatmap)

        # Superimpose the heatmap on original image
        superimposed_img = jet_heatmap * alpha + img
        superimposed_img = tf.keras.preprocessing.image.array_to_img(superimposed_img)

        # Save the superimposed image
        superimposed_img.save(cam_path)

        # Display Grad CAM
        display(Image(cam_path))

    save_and_display_gradcam(img_path, heatmap)
